# ğŸ§­ IROS On-site Challenge

Welcome to the **IROS Vision-Language Navigation On-site Challenge**!
In this phase, participantsâ€™ models will be deployed on **a real robot** to evaluate performance in real-world conditions.

---

## Robot
The robot uses a wheeled chassis. The chassis and camera control code can be found in the provided SDK. The cameraâ€™s default resolution is 640Ã—480, and the depth and color images are already aligned.

The robot is an agilex [RANGER MINI 3.0](https://www.agilex.ai/solutions/1) with RGB camera and a LiDAR sensor.

## âš™ï¸ Installation

First, install the `InternNav` package:

```bash
cd /InternNav
pip install -e .
```

## ğŸš€ Running Your Agent
### 1. Start the Agent Server
Launch your agent server with the desired configuration file:

```bash
python -m internnav.agent.utils.server --config path/to/cfg.py
```

### 2. Test the Agent with Robot Captures
You can locally test your model using previously recorded observations from the robot (stored under ./captures):

```bash
python sdk/test_agent.py --config path/to/cfg.py
```

### 3. Actual Competition Execution
During the on-site evaluation, the organizers will run:

```bash
python sdk/main.py
```

for each episode, paired with its corresponding natural language instruction.

## ğŸ§© Data Format
Action
```python
action = [{'action': [int], 'ideal_flag': bool}]
```
Observation
```python
obs = {
    "rgb": rgb,           # RGB image from the robot
    "depth": depth,       # Depth image (aligned with RGB)
    "instruction": str    # Natural language navigation
}
```

## ğŸ“‹ Rules
Please check out the [onsite competition rules](./onsite_competition_rules_en-US.md) .


## ğŸš€ Code Submission
Submit a Docker image with your agent server preconfigured and ready to run. During the competition, the robot will connect to a local server over the network. Weâ€™ll share additional details soon.
